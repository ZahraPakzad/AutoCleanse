{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "\n",
    "from AutoCleanse.preprocessor import Preprocessor\n",
    "from AutoCleanse.utils import *\n",
    "from AutoCleanse.dataloader import ClfDataset, DataLoader\n",
    "from AutoCleanse.evaluate.classifier import *\n",
    "\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "onehotencoder = OneHotEncoder(sparse_output=False)\n",
    "preprocessor = Preprocessor(scaler,onehotencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../dataset/adult.csv').drop(columns=['fnlwgt'])\n",
    "\n",
    "continous_columns = df.drop(columns=['income']).select_dtypes(include=['int64', 'float64']).columns.tolist() \n",
    "categorical_columns = df.drop(columns=['income']).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "target_columns = ['income']\n",
    "\n",
    "# Seperate features and target\n",
    "X = df[continous_columns+categorical_columns]\n",
    "y = df[target_columns]\n",
    "\n",
    "# Split dataset\n",
    "X_train,X_val,X_test = preprocessor.split(df=X,\n",
    "                                        train_ratio=0.7,\n",
    "                                        val_ratio=0.15,\n",
    "                                        test_ratio=0.15,\n",
    "                                        random_seed=42)\n",
    "X_dirty = replace_with_nan(X_test,0,42)\n",
    "\n",
    "# Fit and transform\n",
    "X_train = preprocessor.fit_transform(input_df=X_train,\n",
    "                                    continous_columns=continous_columns,\n",
    "                                    categorical_columns=categorical_columns)\n",
    "\n",
    "X_val = preprocessor.transform(input_df=X_val,    \n",
    "                            continous_columns=continous_columns,\n",
    "                            categorical_columns=categorical_columns)                          \n",
    "\n",
    "X_test = preprocessor.transform(input_df=X_test,   \n",
    "                                continous_columns=continous_columns,\n",
    "                                categorical_columns=categorical_columns)  \n",
    "\n",
    "X_dirty = preprocessor.transform(input_df=X_dirty,   \n",
    "                                continous_columns=continous_columns,\n",
    "                                categorical_columns=categorical_columns)\n",
    "\n",
    "# Load cleaned test dataset\n",
    "df_cleaned = pd.read_csv(\"../dataset/adult_test_cleaned.csv\")\n",
    "\n",
    "X_cleaned = df_cleaned[continous_columns+categorical_columns]\n",
    "X_cleaned = preprocessor.transform(input_df=X_cleaned,   \n",
    "                                    continous_columns=continous_columns,\n",
    "                                    categorical_columns=categorical_columns)                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess target\n",
    "y_train,y_val,y_test = preprocessor.split(df=y,\n",
    "                                        train_ratio=0.7,\n",
    "                                        val_ratio=0.15,\n",
    "                                        test_ratio=0.15,\n",
    "                                        random_seed=42)\n",
    "y_dirty = replace_with_nan(y_test,0,42)\n",
    "\n",
    "y_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = pd.DataFrame(y_encoder.fit_transform(y_train),columns=y_encoder.get_feature_names_out(target_columns),index=y_train.index)\n",
    "y_val = pd.DataFrame(y_encoder.transform(y_val),columns=y_encoder.get_feature_names_out(target_columns),index=y_val.index)\n",
    "y_test = pd.DataFrame(y_encoder.transform(y_test),columns=y_encoder.get_feature_names_out(target_columns),index=y_test.index)\n",
    "y_dirty = pd.DataFrame(y_encoder.transform(y_dirty),columns=y_encoder.get_feature_names_out(target_columns),index=y_dirty.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClfDataset(X_train, y_train)\n",
    "val_dataset = ClfDataset(X_val, y_val)\n",
    "test_dataset = ClfDataset(X_test, y_test)\n",
    "dirty_dataset = ClfDataset(X_dirty, y_dirty)\n",
    "cleaned_dataset = ClfDataset(X_cleaned, y_test)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    tensor_data = torch.stack([item[0] for item in batch])\n",
    "    # Check if tensor_targets are scalars or tensors\n",
    "    if torch.is_tensor(batch[0][1]):\n",
    "        tensor_targets = torch.stack([item[1] for item in batch])\n",
    "    else:\n",
    "        tensor_targets = torch.tensor([item[1]\n",
    "                                      for item in batch], dtype=torch.float32)\n",
    "    indices = [item[2] for item in batch]\n",
    "    return tensor_data, tensor_targets, indices\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
    "dirty_loader = DataLoader(dirty_dataset, batch_size=batch_size,\n",
    "                          shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
    "cleaned_loader = DataLoader(cleaned_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, drop_last=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [X_train.shape[1], 150, 200, 200, 100, 50]\n",
    "\n",
    "model = ClsNNBase(layers=layers, dropout=[(0, 0.5), (1, 0.5), (2, 0.5)], batch_norm=True, device=device,\n",
    "                  learning_rate=0.025, weight_decay=1e-5, l1_strength=1e-3, l2_strength=1e-3)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model.to(device), torch.tensor(X_train.values).float().to(device).shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                num_epochs=10,\n",
    "                batch_size=batch_size,\n",
    "                layers=layers,\n",
    "                patience=2,\n",
    "                continous_columns=continous_columns,\n",
    "                categorical_columns=categorical_columns,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"local\",\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"local\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(test_loader=test_loader, batch_size=batch_size, device=device)\n",
    "model.test(test_loader=cleaned_loader, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: data anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_train = pd.read_csv(\"../dataset/adult_train_anonymized.csv\",index_col=0)\n",
    "anon_val = pd.read_csv(\"../dataset/adult_val_anonymized.csv\",index_col=0)\n",
    "anon_test = pd.read_csv(\"../dataset/adult_test_anonymized.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_anon = ClfDataset(anon_train, y_train)\n",
    "val_dataset_anon = ClfDataset(anon_val, y_val)\n",
    "test_dataset_anon = ClfDataset(anon_test, y_test)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    tensor_data = torch.stack([item[0] for item in batch])\n",
    "    # Check if tensor_targets are scalars or tensors\n",
    "    if torch.is_tensor(batch[0][1]):\n",
    "        tensor_targets = torch.stack([item[1] for item in batch])\n",
    "    else:\n",
    "        tensor_targets = torch.tensor([item[1]\n",
    "                                      for item in batch], dtype=torch.float32)\n",
    "    indices = [item[2] for item in batch]\n",
    "    return tensor_data, tensor_targets, indices\n",
    "\n",
    "batch_size = 256\n",
    "train_loader_anon = DataLoader(train_dataset_anon, batch_size=batch_size,\n",
    "                               shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
    "val_loader_anon = DataLoader(val_dataset_anon, batch_size=batch_size,\n",
    "                             shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
    "test_loader_anon = DataLoader(test_dataset_anon, batch_size=batch_size,\n",
    "                              shuffle=False, drop_last=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_anon = [anon_train.shape[1], 150, 200, 200, 100, 50]\n",
    "\n",
    "model_anon = ClsNNBase(layers=layers_anon, dropout=[(0, 0.5), (1, 0.5), (2, 0.5)], batch_norm=True, device=device,\n",
    "                       learning_rate=0.025, weight_decay=1e-5, l1_strength=1e-3, l2_strength=1e-3)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Progress: 100%|██████████| 89/89 [00:13<00:00,  6.51it/s]\n",
      "Epoch [1/10], Validation Progress: 100%|██████████| 19/19 [00:02<00:00,  6.95it/s]\n",
      "/home/tung/anaconda3/envs/gpt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss  : 0.55796563, Accuracy: 0.75258954, Precision: 0.46747875, Recall: 0.49926510, F1 Score: 0.43365864\n",
      "Epoch [1/10], Validation Loss: 0.55694846, Accuracy: 0.75637336, Precision: 0.37818668, Recall: 0.50000000, F1 Score: 0.43064497\n",
      "Epoch [1/10]: Learning Rate = [0.025]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Progress: 100%|██████████| 89/89 [00:13<00:00,  6.66it/s]\n",
      "Epoch [2/10], Validation Progress: 100%|██████████| 19/19 [00:02<00:00,  6.89it/s]\n",
      "/home/tung/anaconda3/envs/gpt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss  : 0.55457782, Accuracy: 0.75869031, Precision: 0.37934515, Recall: 0.50000000, F1 Score: 0.43139506\n",
      "Epoch [2/10], Validation Loss: 0.55690311, Accuracy: 0.75637336, Precision: 0.37818668, Recall: 0.50000000, F1 Score: 0.43064497\n",
      "Epoch [2/10]: Learning Rate = [0.025]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Progress: 100%|██████████| 89/89 [00:13<00:00,  6.71it/s]\n",
      "Epoch [3/10], Validation Progress: 100%|██████████| 19/19 [00:02<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss  : 0.55457767, Accuracy: 0.75869031, Precision: 0.37934515, Recall: 0.50000000, F1 Score: 0.43139506\n",
      "Epoch [3/10], Validation Loss: 0.55689693, Accuracy: 0.75637336, Precision: 0.37818668, Recall: 0.50000000, F1 Score: 0.43064497\n",
      "Epoch [3/10]: Learning Rate = [0.025]\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/tung/anaconda3/envs/gpt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_anon.train_model(train_loader=train_loader_anon,\n",
    "                        val_loader=val_loader_anon,\n",
    "                        num_epochs=10,\n",
    "                        batch_size=batch_size,\n",
    "                        layers=layers_anon,\n",
    "                        patience=2,\n",
    "                        continous_columns=continous_columns,\n",
    "                        categorical_columns=categorical_columns,\n",
    "                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weight to ClsNNBase_test_anon.pth\n"
     ]
    }
   ],
   "source": [
    "model_anon.save(\"local\",\"test_anon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anon.load(\"local\", \"test_anon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Progress: 100%|██████████| 19/19 [00:02<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss  : 0.54763939, Accuracy: 0.76562500, Precision: 0.38281250, Recall: 0.50000000, F1 Score: 0.43362832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/tung/anaconda3/envs/gpt/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_anon.test(test_loader=test_loader_anon, batch_size=batch_size, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
